"""
åŸºäºHuggingFaceçš„RAGå¯¹è¯ç³»ç»Ÿ - æ— éœ€API key
"""

# ============================================
# ç¬¬ä¸€éƒ¨åˆ†: å®‰è£…ä¾èµ–
# ============================================
"""
è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–:
pip install langchain sentence-transformers chromadb pypdf transformers torch
"""

# ============================================
# ç¬¬äºŒéƒ¨åˆ†: å¯¼å…¥å¿…è¦çš„åº“
# ============================================
import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ä½¿ç”¨HuggingFaceçš„embeddingæ¨¡å‹
from langchain.embeddings import HuggingFaceEmbeddings

# æ–°å¢: å¯¼å…¥Ollama
from langchain.llms import Ollama

print("âœ… åº“å¯¼å…¥æˆåŠŸ!")
# ============================================
# ç¬¬ä¸‰éƒ¨åˆ†: åŠ è½½å¹¶å¤„ç†æ–‡æ¡£
# ============================================

# 1. åŠ è½½PDFæ–‡æ¡£ (æ›¿æ¢ä¸ºä½ çš„æ–‡æ¡£è·¯å¾„)
print("\nğŸ“„ åŠ è½½æ–‡æ¡£...")
loader = PyPDFLoader("https://arxiv.org/pdf/2309.10305.pdf")  #å…³äº "Baichuan2" æ¨¡å‹çš„è®ºæ–‡ PDF
data = loader.load()

print(f"âœ… åŠ è½½äº† {len(data)} ä¸ªæ–‡æ¡£")

# 2. æ–‡æœ¬åˆ†å‰²
print("\nâœ‚ï¸ åˆ†å‰²æ–‡æ¡£...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # æ¯å—çš„å­—ç¬¦æ•°
    chunk_overlap=50,      # å—ä¹‹é—´çš„é‡å 
    length_function=len,
)

docs = text_splitter.split_documents(data)
print(f"âœ… åˆ†å‰²ä¸º {len(docs)} ä¸ªæ–‡æœ¬å—")

# ============================================
# ç¬¬å››éƒ¨åˆ†: åˆ›å»ºå‘é‡åµŒå…¥(ä½¿ç”¨HuggingFace)
# ============================================

print("\nğŸ”¢ åˆ›å»ºå‘é‡åµŒå…¥æ¨¡å‹...")

# ä½¿ç”¨HuggingFaceçš„å¼€æºembeddingæ¨¡å‹(å…è´¹,æ— éœ€API key)
embed_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    # è¿™æ˜¯ä¸€ä¸ªæ”¯æŒä¸­æ–‡çš„è½»é‡çº§æ¨¡å‹
    model_kwargs={'device': 'cuda'},  # ä½¿ç”¨GPU,å¦‚æœç”¨CPUå¯æ”¹ä¸º'cpu'
    encode_kwargs={'normalize_embeddings': True}
)

print("âœ… Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ!")

# ============================================
# ç¬¬äº”éƒ¨åˆ†: æ„å»ºå‘é‡æ•°æ®åº“
# ============================================

print("\nğŸ’¾ æ„å»ºå‘é‡æ•°æ®åº“...")

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embed_model,  # ä½¿ç”¨HuggingFaceçš„embedding
    collection_name="hf_embed",
    persist_directory="./chroma_db"  # æ•°æ®æŒä¹…åŒ–ç›®å½•
)

print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ!")

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={'k': 3}  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£å—
)

# ============================================
# ç¬¬å…­éƒ¨åˆ†: åŠ è½½Ollamaè¯­è¨€æ¨¡å‹
# ============================================

print("\nğŸ¤– åŠ è½½è¯­è¨€æ¨¡å‹...")

# æ–¹æ¡ˆ1: ä½¿ç”¨Ollama(æ¨è!ç®€å•å¿«é€Ÿ)
try:
    llm = Ollama(
        model="qwen:1.8b",      # ä½¿ç”¨Qwen 1.8Bä¸­æ–‡æ¨¡å‹
        # å…¶ä»–å¯é€‰æ¨¡å‹:
        # model="llama2:7b"     # è‹±æ–‡æ¨¡å‹
        # model="qwen:7b"       # æ›´å¤§çš„ä¸­æ–‡æ¨¡å‹
        temperature=0.7,        # æ§åˆ¶åˆ›é€ æ€§(0-1)
    )
    
    # æµ‹è¯•Ollamaæ˜¯å¦æ­£å¸¸å·¥ä½œ
    test_response = llm("ä½ å¥½")
    print("âœ… Ollamaè¯­è¨€æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"   æµ‹è¯•å›å¤: {test_response[:50]}...")
    
except Exception as e:
    print(f"âš ï¸ OllamaåŠ è½½å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·ç¡®ä¿:")
    print("   1. å·²å®‰è£…Ollama (è®¿é—® https://ollama.ai)")
    print("   2. å·²ä¸‹è½½æ¨¡å‹ (è¿è¡Œ: ollama pull qwen:1.8b)")
    print("   3. OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")
    print("\nğŸ’¡ å°†ä½¿ç”¨ä»…æ£€ç´¢æ¨¡å¼...")
    llm = None


# ============================================
# ç¬¬ä¸ƒéƒ¨åˆ†: åˆ›å»ºRAGé—®ç­”é“¾
# ============================================

print("\nğŸ”— åˆ›å»ºé—®ç­”é“¾...")

# å®šä¹‰æç¤ºæ¨¡æ¿
template = """è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆ,è¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡: {context}

é—®é¢˜: {question}

å›ç­”:"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

if llm:
    # åˆ›å»ºå®Œæ•´çš„é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    print("âœ… é—®ç­”é“¾åˆ›å»ºæˆåŠŸ!")
else:
    print("âš ï¸ ä»…ä½¿ç”¨æ£€ç´¢åŠŸèƒ½æ¼”ç¤º")

# ============================================
# ç¬¬å…«éƒ¨åˆ†: æµ‹è¯•é—®ç­”ç³»ç»Ÿ
# ============================================

def ask_question(question):
    """æé—®å‡½æ•°"""
    print(f"\nâ“ é—®é¢˜: {question}")
    print("-" * 60)
    
    if llm:
        # â­â­â­ ä½¿ç”¨å®Œæ•´çš„RAGç³»ç»Ÿ(æœ‰Ollama) â­â­â­
        result = qa_chain({"query": question})
        
        # æ˜¾ç¤ºAIç”Ÿæˆçš„å›ç­”
        print(f"\nğŸ’¡ AIå›ç­”:\n{result['result']}")
        
        # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
        print(f"\nğŸ“š å‚è€ƒæ¥æº:")
        for i, doc in enumerate(result['source_documents'], 1):
            print(f"{i}. {doc.page_content[:100]}...")
    else:
        # ä»…æ¼”ç¤ºæ£€ç´¢åŠŸèƒ½(æ²¡æœ‰Ollama)
        docs = retriever.get_relevant_documents(question)
        print("ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
        for i, doc in enumerate(docs, 1):
            print(f"\n{i}. {doc.page_content}")

# ============================================
# ç¬¬ä¹éƒ¨åˆ†: è¿è¡Œç¤ºä¾‹
# ============================================

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸš€ RAGå¯¹è¯ç³»ç»Ÿå·²å°±ç»ª!")
    print("=" * 60)
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "How large is the baichuan2 vocabulary?",
        "ä½ çŸ¥é“baichuan2æ¨¡å‹å—ï¼Ÿ",
        "å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆ?"
    ]
    
    for q in questions:
        ask_question(q)
        print("\n")
    
    # äº¤äº’å¼é—®ç­”
    print("\nğŸ’¬ ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†!(è¾“å…¥'é€€å‡º'ç»“æŸ)")
    while True:
        question = input("\nä½ çš„é—®é¢˜: ").strip()
        if question.lower() in ['é€€å‡º', 'exit', 'quit', 'q']:
            print("ğŸ‘‹ å†è§!")
            break
        if question:
            ask_question(question)